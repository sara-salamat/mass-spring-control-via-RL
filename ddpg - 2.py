# -*- coding: utf-8 -*-
"""ddpg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o4vLgw-JnpwwYyK7-HPDy9AZ8-ZWLzuT
"""

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
print(tf.__version__)

# envirtonment parameters
m = 1/1.567
k = 32.93/1.567
c = 8.883/1.567
A = np.array([[0 , 1] , [-k/m , -c/m]]).astype(np.float32())
B = np.array([[0],[1/m]]).astype(np.float32())
Phi = np.array([[.9996 , .0049] , [ -0.1610 , .9562]]).astype(np.float32())
Gamma = np.array([[1.929937854151416e-05],[0.0077]]).astype(np.float32())

T = 0.005 # Sampling time
tau = 0.01 # target network updating parameter
discount_factor = 0.99
alpha = 0.6
beta = 0.4
K_min = [0.01]*3
K_max = [2 , 2 , 4]
epochs = 1000

layer1 = 400
layer2 = 300

upper_bound = 10 # upper bound for u (force applied to the mass)
lower_bound = -10 # lower bound for u (force applied to the mass)

xref = 0.005 # reference!

class env():
    def __init__(self):
        self.Phi = Phi
        self.Gamma = Gamma
        self.xref = xref
        self.x = 0
        self.xdot = 0
        
        self.nninput = np.array([self.x ,self.xdot ,self.xref ]) # input fed into Actor and Critic neural nets (states)
        self.num_states = self.nninput.shape[0]
        self.num_actions = 3 # Ki , Kp, Kd (outputs of actor network)
    def reset(self):
        self.xdot = 0
        self.nninput = np.array([self.x ,self.xdot ,self.xref])

        return np.array([[self.x] , [self.xdot]])
    def step(self , action , xref):
        self.xref = xref
        a = np.dot(self.Phi , np.array([[self.x] , [self.xdot]])) + np.dot(self.Gamma , action).reshape(2,1)
        self.x = a[0,0]
        self.xdot = a[1,0]
        if np.abs(self.xref - self.x)>0.0005:
            reward = -100*np.abs(self.xref - self.x)
#            done = False
        else:
            reward = +1
#            done = True
        self.nninput = np.array([self.x,self.xdot , self.xref])
        return np.array([[self.x] , [self.xdot]]), reward
    def is_done(self , i_p):
      
      if np.abs(self.xref - self.x)<0.0005:
        i_p = i_p + 1
      else:
        i_p = 0
      if i_p >=3000:
        done = True
      else:
        done = False
      return done , i_p

en = env()
num_states = en.num_states # states are: x(t), reference(t) - x(t)
num_actions = en.num_actions

############################################################
m = 1/1.567
m = 2*m
k = 32.93/1.567
c = 8.883/1.567
A = np.array([[0 , 1] , [-k/m , -c/m]]).astype(np.float32())
B = np.array([[0],[1/m]]).astype(np.float32())
Phi = np.array([[0.999795709603365 , 0.004944550725663] , [ -0.081412027698044 , 0.977834487555332]]).astype(np.float32())
Gamma = np.array([[9.721319511894614e-06],[0.003874055493557]]).astype(np.float32())
class env2():
    def __init__(self):
        self.Phi = Phi
        self.Gamma = Gamma
        self.xref = xref
        self.x = 0
        self.xdot = 0
        
        self.nninput = np.array([self.x ,self.xdot ,self.xref ]) # input fed into Actor and Critic neural nets (states)
        self.num_states = self.nninput.shape[0]
        self.num_actions = 3 # Ki , Kp, Kd (outputs of actor network)
    def reset(self):
        self.xdot = 0
        self.nninput = np.array([self.x ,self.xdot ,self.xref])

        return np.array([[self.x] , [self.xdot]])
    def step(self , action , xref):
        self.xref = xref
        a = np.dot(self.Phi , np.array([[self.x] , [self.xdot]])) + np.dot(self.Gamma , action).reshape(2,1)
        self.x = a[0,0]
        self.xdot = a[1,0]
        if np.abs(self.xref - self.x)>0.0005:
            reward = -100*np.abs(self.xref - self.x)
#            done = False
        else:
            reward = +1
#            done = True
        self.nninput = np.array([self.x,self.xdot , self.xref])
        return np.array([[self.x] , [self.xdot]]), reward
    def is_done(self , i_p):
      
      if np.abs(self.xref - self.x)<0.0005:
        i_p = i_p + 1
      else:
        i_p = 0
      if i_p >=5000:
        done = True
      else:
        done = False
      return done , i_p

en2 = env2()
############################################################

def get_actor():

    # input layer
    inputs = layers.Input(shape=(num_states,))
    # first layer
    out = layers.Dense(layer1, activation="relu")(inputs)
    # second layer
    out = layers.Dense(layer2, activation="relu")(out)
    out = layers.Dense(250, activation="relu")(out)
    out = layers.Dense(200, activation="relu")(out)
    # output layer
    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003) # Initialize weights between -3e-3 and 3-e3
    outputs = layers.Dense(num_actions,activation=None, kernel_initializer=last_init)(out)


    # Our upper bound is 2.0 for Pendulum.
    model = tf.keras.Model(inputs, outputs)
    return model


def get_critic():
    # placeholders for states and actions
    state_input = layers.Input(shape=(num_states,))
    action_input = layers.Input(shape=(num_actions,))

    #first hidden layer with states as inputs
    state_out = layers.Dense(layer1, activation="relu" )(state_input)

    # second hidden layer with actions and output of first layer as input | Both are passed through seperate layer before concatenating
    state_l2 = layers.Dense(layer2 )(state_out)
    action_l2 = layers.Dense(layer2)(action_input)

    concat = layers.Add()([state_l2, action_l2])
    mid = tf.keras.activations.relu(concat, alpha=0) #leaky relu

    outputs = layers.Dense(250 )(mid) #ezafe
    outputs = layers.Dense(200 )(outputs)# ezafe

    # output layer
    outputs = layers.Dense(1)(outputs)

    # Outputs single value for given state-action
    model = tf.keras.Model([state_input, action_input], outputs)

    return model

@tf.function
def update_target(target_weights, weights, tau):
    for (a, b) in zip(target_weights, weights):
        a.assign(b * tau + a * (1 - tau))

def PID(et, et1, et2, action, u0):
    Kp = action[0] # Ki
    Ti = action[1] # Kp
    Td = action[2] # Kd
    
#    k1 = Kp*(1 + T/Ti + Td/T)
 #   k2 =-Kp*(1+2*Td/T)
  #  k3 = Kp*(Td/T)
    
    # et = error at time t
    # et1 = error at time t-1
    # et2 = error at time t-2
    e1 = et
    e2 = et - et1 
    e3 = et - 2*et1 + et2
    

    u = u0 + Kp*e1 + Ti*e2 + Td*e3
    #u = u0 + k1*et + k2*et1 + k3*et2
 #   u = Kp*e1 + Ti*e2 + Td*e3
    return u

def policy(state, noise_object , et , et1 , et2 , u0):
    sampled_actions = tf.squeeze(actor_model(state))
    sampled_actions = tf.clip_by_value(sampled_actions , K_min , K_max)
    noise = noise_object()
    # Adding noise to action
#    sampled_actions = sampled_actions.numpy() + max(4.*epsilon,0)*noise
    sampled_actions = sampled_actions.numpy() + noise
    sampled_actions = np.clip(sampled_actions , K_min , K_max)
    u = PID(et, et1, et2, sampled_actions, u0)
    

    # We make sure action is within bounds
    legal_action = np.clip(u, lower_bound, upper_bound)

    return [np.squeeze(legal_action)] , np.squeeze(sampled_actions - noise)

class OUActionNoise:
    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):
        self.theta = theta
        self.mean = mean
        self.std_dev = std_deviation
        self.dt = dt
        self.x_initial = x_initial
        self.reset()

    def __call__(self):
        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.
        x = (
            self.x_prev
            + self.theta * (self.mean - self.x_prev) * self.dt
            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)
        )
        # Store x into x_prev
        # Makes next noise dependent on current one
        self.x_prev = x
        return x

    def reset(self):
        if self.x_initial is not None:
            self.x_prev = self.x_initial
        else:
            self.x_prev = np.zeros_like(self.mean)

class Buffer:
    def __init__(self, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0
        
        self.max_action = 8# [1 ,1 ,1]
        self.min_action = 0

        # Instead of list of tuples as the exp.replay concept go
        # We use different np.arrays for each tuple element
        self.state_buffer = np.zeros((self.buffer_capacity, num_states))
        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))
        self.reward_buffer = np.zeros((self.buffer_capacity, 1))
        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity

        self.state_buffer[index] = obs_tuple[0].astype(np.float32())
        self.action_buffer[index] = obs_tuple[1].astype(np.float32())
        self.reward_buffer[index] = obs_tuple[2]
        self.next_state_buffer[index] = obs_tuple[3].astype(np.float32())

        self.buffer_counter += 1

    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows
    # TensorFlow to build a static graph out of the logic and computations in our function.
    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.
    
    def update(
        self, state_batch, action_batch, reward_batch, next_state_batch
    ):
        # Training and updating Actor & Critic networks.
        # See Pseudo Code.
        with tf.GradientTape() as tape:
            target_actions = target_actor(next_state_batch, training=True)
            y = reward_batch + discount_factor * target_critic(
                [next_state_batch, target_actions], training=True
            )
            critic_value = critic_model([state_batch, action_batch], training=True)
            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value)) # Mean Squared Error

        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)
        
        critic_optimizer.apply_gradients(
            zip(critic_grad, critic_model.trainable_variables)
        )

        
        
        with tf.GradientTape() as tape:
            actions = actor_model(state_batch, training=True)
            #critic_value = critic_model([state_batch, actions], training=True)
            # Used `-value` as we want to maximize the value given
            # by the critic for our actions
            #actor_loss = -tf.math.reduce_mean(critic_value)
        a_gradients = action_gradient(state_batch , action_batch) # dQ/dK
        #a_gradients = tf.cast(a_gradients, dtype=tf.float32)
        dq_da = invert_gradients(a_gradients , action_batch)
        '''

        for i in range(self.batch_size):
          for j in range(num_actions):
            if a_gradients[i][j]>= 0.0:
              dq_da[i][j] = a_gradients[i][j]*(K_max - action_batch[i][j])/(K_max - K_min)
            else:
              dq_da[i][j] = a_gradients[i][j]*(action_batch[i][j] - K_min)/(K_max - K_min)
        '''
        
        a_gradients = tf.convert_to_tensor(dq_da , dtype = tf.float32)
        #a_gradients = tf.cast(a_gradients, dtype=tf.float32)   
        unnormalized_actor_grad = tape.gradient(actions, actor_model.trainable_variables, output_gradients = -a_gradients)
        actor_grad = list(map(lambda x: tf.math.divide(x, self.batch_size), unnormalized_actor_grad))
        
        actor_optimizer.apply_gradients(
            zip(actor_grad, actor_model.trainable_variables)
        )
        #actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)

    # We compute the loss and update parameters
    def learn(self):
        # Get sampling range
        record_range = min(self.buffer_counter, self.buffer_capacity)
        # Randomly sample indices
        batch_indices = np.random.choice(record_range, self.batch_size)
        

        # Convert to tensors
        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])
        state_batch = tf.cast(state_batch, dtype=tf.float32)
        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])
        action_batch = tf.cast(action_batch, dtype=tf.float32)
        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])
        reward_batch = tf.cast(reward_batch, dtype=tf.float32)
        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])
        next_state_batch = tf.cast(next_state_batch, dtype=tf.float32)
        

        self.update(state_batch, action_batch, reward_batch, next_state_batch)

def invert_gradients(a_gradients , action_batch):
  dq_da = np.zeros((batch_size , num_actions))
  for i in range(batch_size):
    for j in range(num_actions):
      if a_gradients[i][j]>= 0.0:
        dq_da[i][j] = a_gradients[i][j]*(K_max[j] - action_batch[i][j])/(K_max[j] - K_min[j])
      else:
        dq_da[i][j] = a_gradients[i][j]*(action_batch[i][j] - K_min[j])/(K_max[j] - K_min[j])
  return dq_da

def action_gradient(state_batch , action_batch):
  #var = tf.constant(action_batch)
  with tf.GradientTape(watch_accessed_variables=False) as tape:
    tape.watch(action_batch)
    prediction = critic_model([state_batch, action_batch])
  return tape.gradient(prediction, action_batch)

# loop initializing
std_dev = 0.2
ou_noise = OUActionNoise(mean=np.zeros(num_actions), std_deviation=float(std_dev)*np.ones(num_actions))

actor_model = get_actor()
critic_model = get_critic()

target_actor = get_actor()
target_critic = get_critic()
# Making the weights equal initially
target_actor.set_weights(actor_model.get_weights())
target_critic.set_weights(critic_model.get_weights())

critic_lr = 0.0001
actor_lr = 0.00001

critic_optimizer = tf.keras.optimizers.Adam(critic_lr)
actor_optimizer = tf.keras.optimizers.Adam(actor_lr)

epochs = 100

batch_size = 64
buffer = Buffer(100000, batch_size)

xref_list = [0.05 , -0.005 , 0.01 ,-0.002 , 0.05 , -0.005 , 0.01 ,-0.002, 0.1, 0.3 , 0.001] 
#xref_list = [0.05]

# loop

# To store reward history of each episode
ep_reward_list = []
# To store average reward history of last few episodes
avg_reward_list = []
#trajectory = [0.005]*int(epochs/2) + [-0.005]*int(epochs/2)
#trajectory = [0.005]*int(epochs)

K_log = [] # to store K in the whole process
x_log = []
xref_log = []
chnge = 5
for ep in range(len(xref_list)):

    xref = xref_list[ep]
    x_errors = [xref , xref ,xref] # this should be changed
    if ep>chnge:
        prev_state = en2.reset()
        prev_state_nn = en2.nninput
    else:
        prev_state = en.reset()
        prev_state_nn = en.nninput
    episodic_reward = 0
    action = 0
    
    p_for_loop = 0
    inloop_x = []
    inloop_K = []
    xref_log.append(xref)
    
    i_p = 0
    while True:
        # Uncomment this to see the Actor in action
        # But not in a python notebook.
        # env.render()
        p_for_loop = p_for_loop + 1
        
        if p_for_loop % 50 == 0:
          is_training = True
          
        else:
          is_training = False
        
        #is_training = True
        print('episode: ' + str(ep) + ' | inner loop number: ' + str(p_for_loop) + ' xref = ' + str(xref))
        print('** ' + str(is_training))

        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state_nn), 0)
        x_errors = [x_errors[1] , x_errors[2] ,(xref-prev_state[0])[0] ]
        

        action , ks = policy(tf_prev_state, ou_noise , x_errors[0] , x_errors[1] , x_errors[2] , action) # u & K
        print(ks)
        inloop_K.append(ks)
        # Recieve state and reward from environment
        if ep>chnge:
            state, reward = en2.step(action , xref)
            state_nn = en2.nninput

        else:
            state, reward = en.step(action , xref)
            state_nn = en.nninput

        inloop_x.append(state[0][0])
        print(reward)
        

        buffer.record((prev_state_nn.astype(dtype = np.float32), ks.astype(dtype = np.float32), reward, state_nn.astype(dtype = np.float32)))
        episodic_reward += reward
        if buffer.buffer_counter > batch_size and is_training:
            
            buffer.learn()
            update_target(target_actor.variables, actor_model.variables, tau)
            update_target(target_critic.variables, critic_model.variables, tau)
        
        if ep>chnge:
            done, i_p = en2.is_done(i_p)
        else:
            done, i_p = en.is_done(i_p)

        
        # End this episode when `done` is True
        if done or p_for_loop > 500000:
            break

        prev_state = state
        prev_state_nn = state_nn

    ep_reward_list.append(episodic_reward)
    x_log.append(inloop_x)
    K_log.append(inloop_K)

    # Mean of last 40 episodes
    avg_reward = np.mean(ep_reward_list[-40:])
#    print("Episode * {} * Avg Reward is ==> {}0".format(ep, avg_reward))
    avg_reward_list.append(avg_reward)

'''
episode = 90 # no xdot = 0, training at ep 250
fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)
ax1.plot([i*T for i in range(len(x_log[episode]))] , x_log[episode])
ax1.plot([i*T for i in range(len(x_log[episode]))] , [xref_log[episode] for i in range(len(x_log[episode]))])
ax1.set_title('displacement in epoch number ' + str(episode))
ax1.set(xlabel='time(s)', ylabel='x(m)')
ax2.plot([i*T for i in range(len(K_log[episode]))] , [m[0] for m in K_log[episode]])
ax2.plot([i*T for i in range(len(K_log[episode]))] ,[m[1] for m in K_log[episode]])
#ax2.plot([i*T for i in range(len(K_log[episode]))] ,[m[2] for m in K_log[episode]])
ax2.set_title('K in epoch number ' + str(episode))
ax2.set(xlabel='time(s)')
ax2.legend(['Ki' , 'Kp' , 'Kd'])
print(xref_log[episode])
'''



totalsteps = 0
total_x = []
refs = []
for m in range(len(x_log)):
    totalsteps = totalsteps + len(x_log[m])
    total_x = total_x + x_log[m]
    refs = refs + [xref_list[m]]*len(x_log[m])
totaltime = [i*T for i in range(totalsteps)]
ki = []
kp = []
kd = []

for i in range(len(xref_list)):
    for j in range(len(K_log[i])):
        ki.append(K_log[i][j][0])
        kp.append(K_log[i][j][1])
        kd.append(K_log[i][j][2])


fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)
ax1.plot(totaltime , total_x)
ax1.plot(totaltime , refs)
ax1.set_title('displacement')
ax1.set(xlabel='time(s)', ylabel='x(m)')

ax2.plot(totaltime , ki)
ax2.plot(totaltime , kp)
ax2.plot(totaltime , kd)
ax2.set_ylim([0,2])
ax2.legend(['Ki' , 'Kp' , 'Kd'])



